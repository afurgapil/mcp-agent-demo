# ðŸ”§ GLOBAL CONFIG
SYSTEM_PROMPT = "You are an expert SQL engineer. Receive natural language questions together with the database schema and reply with a SQL statement that can be executed directly against the database. Output only the SQL statement without explanations or markdown fences. Use the schema exactly as provided."

MAX_TOKENS = 2048
TEMPERATURE= 0
TOP_P=0.8
REPEAT_PENALTY=1.5

# ðŸ”§ FastAPI App
app = FastAPI(
    title="GGUF SQL API",
    version="1.0",
    description="GGUF tabanlÄ± LLM ile SQL Ã¼retim API'si",
)

# ðŸ”§ GGUF Model Path (DeÄŸiÅŸtirilebilir)
GGUF_MODEL_PATH = "/home/barfas/Desktop/sql-model/models/gpt-oss-20b-Q5_K_M.gguf"

# ðŸš€ Model yÃ¼klemesi
logger.info("ðŸ”„ GGUF Model yÃ¼kleniyor...")
llm = Llama(
    model_path=GGUF_MODEL_PATH,
    n_ctx=16384,
    n_threads=8, # CPU thread sayÄ±sÄ± (gerekirse arttÄ±r)
    n_gpu_layers=20, # GPU hÄ±zlandÄ±rma (eÄŸer destekliyorsa)
    verbose=False
)
logger.info("âœ… GGUF model yÃ¼klendi.")

# ðŸ“¥ Ä°stek modeli
class GenerateRequest(BaseModel):
    message: str = Field(..., example="Son 30 gÃ¼n iÃ§inde alÄ±ÅŸveriÅŸ yapan mÃ¼ÅŸteriler kimler?")
    system_prompt: Optional[str] = Field(None, description="Ä°stek bazlÄ± sistem prompt")
    schema: Optional[str] = Field(None, description="Ä°stek bazlÄ± DB ÅŸemasÄ± (JSON veya metin)")

# ðŸ“¤ YanÄ±t modeli
class ChatResponse(BaseModel):
    response: str

# ðŸ“¥ Config gÃ¼ncelleme modeli
class UpdateConfigRequest(BaseModel):
    system_prompt: Optional[str] = None
    schema: Optional[str] = None

# ðŸ“¤ Config yanÄ±t modeli
class ConfigResponse(BaseModel):
    system_prompt: str
    schema: str

# âœ… SQL Ãœretimi
@app.post("/api/generate", response_model=ChatResponse, tags=["Chat"])
def generate_sql(req: GenerateRequest):
    logger.info("ðŸ“¨ Yeni SQL isteÄŸi: %s", req.message)
    active_schema = req.schema if (req.schema and req.schema.strip()) else DB_SCHEMA
    active_system_prompt = (
        req.system_prompt if (req.system_prompt and req.system_prompt.strip()) else SYSTEM_PROMPT
    )
    logger.info("ðŸ“¨ Aktif ÅŸema uzunluÄŸu: %d", len(active_schema or ""))
    logger.info("ðŸ“¨ Aktif system prompt uzunluÄŸu: %d", len(active_system_prompt or ""))
    prompt = f"""<|system|>
{active_system_prompt}

<|user|>
Soru:
{req.message}

{active_schema}
<|assistant|>"""

    start = time.perf_counter()

    output = llm(
        prompt,
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        top_p=TOP_P,
        repeat_penalty=REPEAT_PENALTY,
        stop=["<|user|>", "<|system|>"]
    )

    end = time.perf_counter()
    result = output["choices"][0]["text"]
    result = re.sub(r"^(assistant|Assistant):", "", result, flags=re.IGNORECASE).strip()

    logger.info("âœ… YanÄ±t sÃ¼resi: %.2f ms", (end - start) * 1000)
    return {"response": result}

